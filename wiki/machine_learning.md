### 梯度
梯度方向是使函数值增大最快的方向（山坡高度变高， 温度变热）， 梯度值则告诉我们到底有多陡
以 y = x * x - x 为例， 我们知道极小值在x  = 1 / 2 处,  
    y’ = 2 * x - 1
    假设从初始值x0 = 5处开始搜索， y’ | _(x = 5) = 9, 意味着x轴正方向会使函数值升高， 应该往x轴负方向搜索
    假设从初始值x0 = 0处开始搜索， y’ | _(x = 0) = -1, 意味着x轴负方向会使函数值升高，应该往x轴正方向搜索
    迭代的时候应该加上负梯度，即 x := x - alpha * y’

### 最小二乘
```
L(w) = \sum (f(x) - y)^2
dL / dw = (wx - y ) * x
```

### 逻辑斯蒂
逻辑斯蒂用于解决分类问题而不是回归问题
已知 y 属于[0, 1] ，那么希望预测出来的值`h_\theta(x)` 在0-1之间,是一个概率
```
h_\theta(x) = 1 / (1 + e ^ (\theta * x ))
```
它的(交叉熵)损失函数是 `h ^ y  * (1 - h) ^ (1- y)` , 要最大化所有样本上的对数似然
```
y * log(h) + (1- y) * log(1 - h)
```
推一下就知道
```
h’ = x * h * (1 - h)
```
参数更新 `theta := theta + alpha * (y - h) * x`
```
L(w) = y log f(x) + (1-y) log (1 - f(x))
dL / dw = (y - f(x)) * x
```
形式上和最小二乘法的参数更新公式很像

### 决策树
决策树（这里指分类树）是要找一个树形结构的分类器，根据样本属性决定划分的路径，叶子节点的标签即为决策
建树过程是按照某一优先级（拆分该属性后，能有效提纯样本）递归地拆分属性，
属性有多少种取值则增加多少分支
遇到三种情况时递归结束：1）属性取值唯一（取最多作为标签，该节点的后验）；2）该分支样本已纯（标签唯一）；3）该分支没有样本了（取最多作为标签，父节点的先验）

拆分方法：
1. 信息增益：熵可以表示纯度，熵越小，纯度越高；拆分某个属性获得的信息增益 = 当前节点的熵 - 拆分后新产生的各节点的熵的加权平均 g(D, A) = H(D) - H(D|A)。 信息增益准则会偏好取值多的属性，增益率(增益  / 属性的熵 ) 准则会偏好取值少的属性，可以先选增益高于平均水平的属性，再选出增益率最高的属性。
2. 基尼系数：随机抽两个样本，两个样本不一样的概率，即 1 - \sum (pi * pi), pi 为 属性取值为i的频率

## xg-boost
gbdt (gradient boost decision tree)的优化版, 回归树
回归树根据属性和切分阈值将样本分配到某个叶子节点上，
每个叶子节点都会有一个预测值，若叶子节点的样本值不唯一，则输出平均值 (误差度量：最小二乘)
boosting: 下一棵树要拟合的目标是上一棵树的残差
>为啥gbdt所需树的深度较浅， 而随机森林所需的树的深度较深
    因为gbdt使用boosting方法，即下一次调参依赖于上次的残差,关注如何减小variance，即如何拟合数据
    随机森林使用bagging方法，即独立训练好多棵树，关注如何减小bias，即尽量保证泛化能力
    故gbdt会适合浅层树，而随机森林会适合深层树

### svm
#### 推导
首先，希望函数距离越大越好
max gamma
s.t. yi * (w * xi + b) >= gamma
这样有问题，可调w让gamma正无穷
改为希望几何间隔越大越好，物理意义：最大化点到分界面的距离（点到直线的距离|wx+b| / ||w||）
max gamma / ||w||
让gamma强制为1，就变成了
max 1 / ||w||
s.t. yi * (w * xi + b) >= 1

> svm推导里为啥令\gamma=1:
    这其实是一个隐含的缩放约束
    对于超平面`wx + b=0`, 同时缩放`w`和`b`不会改变超平面的位置，即不会改变待解决的问题
    `\gamma = wx^(i) + b`, 假设已经求到了`w`和`b`，也知道对于特定的`x^(i)`的函数间隔functional margin gamma
    两边同时除以除以gamma的数值，即可满足约束。这样可以让优化问题的函数和约束都为凸函数。

但是这样就变成非凸问题了，解决办法:
```
min 1 / 2 ||w||^2
s.t. yi * (w * xi + b) >= 1
```
拉格朗日乘子法
```
L(w,b,a) = 1 / 2 ||w||^2 + \sum [1- ai * yi * (w + xi + b)]
```
考虑`theta(a) = max_a: 1 / 2 ||w||^2 + \sum [1- ai * yi * (w + xi + b)]` = {如果满足约束 => 1 / 2 ||w||^2 , 不满足约束 => 正无穷}
所以原问题等价于`min_{w,b} theta(a) = min_{w,b} max_a : 1 / 2 ||w||^2 + \sum [1- ai * yi * (w + xi + b)]` 
对偶问题往往比原问题更好求解（因为可以直接对w, b求偏导，令其为0并带入化简）
对偶问题为
```
max_a min_{w,b} : 1 / 2 ||w||^2 + \sum [1- ai * yi * (w + xi + b)]
```
对w偏导的偏导为0 => w = \sum ai yi xi
对b偏导为0 =>   \sum ai yi = 0
带入化简为
```
max \sum ai + 0.5 \sum_i \sum_j ai aj yi yj <xi, xj>
s.t. \sum ai yi = 0
```
这是一个二次规划问题,可以解了
#### 核函数
#### 软间隔


### 防止过拟合的方法
- 增加数据
- 控制模型空间
正则：加先验
eary-stop
- 增加噪声
dropout: 引入随机噪声
bagging: 取多个模型的平均作为结果
