### CNN代数
l input
k kernel size
p padding
s stride
输出o和输入l的关系
o = (l + 2p - k) / s + 1
解释：以一维卷积为例，先两边分别补padding, 再把kenel的长度减掉，输入序列中不含kernel的那部分(input[k+1:])可以直接除步长，而kernel的那部分（input[:k]）恰好对应输出中多出来的一项，所以之后+1

### 反卷积是什么
例如 `4*4`的input，`3*3`的kernel，输出`2*2` ( 4 - 3 + 1 = 2)
将`4*4`拉长成`16*1`的X， `2*2`拉长成`4*1`的Y，
kernel定义了`4 * 16`的共享权值的稀疏矩阵C
卷积的正向过程可以看成仿射变换`Y = C * X`, `(4*1) = (4*16) * (16*1)`
反向过程可以看成 `X = C ^T * Y`, `(16*1) = (16*4) * (4*1) `
反卷积就是正向过程左乘C^T, 反向过程左乘C

### Relu
置0是为了稀疏性，线性是为了防止误差反向传播时梯度消失

## RNN
两个方向二维展开
```
x_l^t = tanh ( w_l * [x_{l-1}^t, x_l^{t-1}]’  )
```
LSTM：引入记忆,学会遗忘,解决梯度消失或爆炸问题

## GAN
```
L = y log( D(x) ) + (1 - y) log ( 1- D(G(z)))
```
